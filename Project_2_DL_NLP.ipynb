{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning for NLP - Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RULES:\n",
    "\n",
    "* Do not create any additional cell\n",
    "\n",
    "* Fill in the blanks\n",
    "\n",
    "* All cells should be runnable (modulo trivial compatibility bugs that we'd fix)\n",
    "\n",
    "* 4 / 20 points will be allocated to the clarity of your code\n",
    "\n",
    "* Efficient code will have a bonus\n",
    "\n",
    "DELIVERABLE:\n",
    "\n",
    "* this notebook\n",
    "* the predictions of the SST test set\n",
    "\n",
    "DO NOT INCLUDE THE DATASETS IN THE DELIVERABLE.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "import numpy as np\n",
    "import scipy\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PATH_TO_DATA = \"C:/Users/Chuyi/Documents/DSBA/T2/Deep_Learning/Mini_project_2/nlp_project/data/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Monolingual (English) word embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Word2vec():\n",
    "    def __init__(self, fname, nmax=100000):\n",
    "        self.load_wordvec(fname, nmax)\n",
    "        self.word2id = dict.fromkeys(self.word2vec.keys())\n",
    "\n",
    "        self.id2word = {v: k for k, v in self.word2id.items()}\n",
    "        self.embeddings = np.array(self.word2vec.values())\n",
    "        \n",
    "        index=0\n",
    "        for i in self.word2id.keys():\n",
    "            self.word2id[i] = index\n",
    "            index += 1\n",
    "    \n",
    "    def load_wordvec(self, fname, nmax):\n",
    "        self.word2vec = {}\n",
    "        with io.open(fname, encoding='utf-8') as f:\n",
    "            next(f)\n",
    "            for i, line in enumerate(f):\n",
    "                word, vec = line.split(' ', 1)\n",
    "                #self.word2vec[word.lower()] = np.fromstring(vec, sep=' ')\n",
    "                self.word2vec[word] = np.fromstring(vec, sep=' ')\n",
    "                if i == (nmax - 1):\n",
    "                    break\n",
    "        print('Loaded %s pretrained word vectors' % (len(self.word2vec)))\n",
    "\n",
    "\n",
    "    def most_similar(self, w, K=5):\n",
    "        # K most similar words: self.score  -  np.argsort \n",
    "        scores = []\n",
    "        for word_comp in self.word2vec.keys():\n",
    "            scores.append(self.score(w,word_comp))\n",
    "        scores_array = np.asarray(scores)\n",
    "\n",
    "        idxs = scores_array.argsort()[::-1][1:K+1]\n",
    "\n",
    "        key = list(self.word2vec.keys())\n",
    "        similar_words = []\n",
    "        for i in idxs:\n",
    "            similar_words.append(key[i])\n",
    "        return similar_words\n",
    "\n",
    "    def score(self, w1, w2):\n",
    "        vect1= self.word2vec[w1]\n",
    "        vect2= self.word2vec[w2]\n",
    "        dot_product = np.dot(vect1, vect2)\n",
    "        norm_w1 = np.linalg.norm(vect1)\n",
    "        norm_w2 = np.linalg.norm(vect2)\n",
    "        return dot_product/(norm_w1*norm_w2)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 200000 pretrained word vectors\n",
      "cat dog 0.671683666279\n",
      "dog pet 0.684206402967\n",
      "dogs cats 0.707438932805\n",
      "paris france 0.777510854129\n",
      "germany berlin 0.7420295236\n",
      "['cats', 'kitty', 'kitten', 'feline', 'kitties']  are the most similar words for:  cat\n",
      "['dogs', 'puppy', 'Dog', 'doggie', 'canine']  are the most similar words for:  dog\n",
      "['dog', 'pooches', 'Dogs', 'doggies', 'canines']  are the most similar words for:  dogs\n",
      "['france', 'Paris', 'parisian', 'london', 'berlin']  are the most similar words for:  paris\n",
      "['austria', 'europe', 'german', 'berlin', 'poland']  are the most similar words for:  germany\n"
     ]
    }
   ],
   "source": [
    "w2v = Word2vec(os.path.join(PATH_TO_DATA, 'crawl-300d-200k.vec'), nmax=200000)\n",
    "\n",
    "# You will be evaluated on the output of the following:\n",
    "for w1, w2 in zip(('cat', 'dog', 'dogs', 'paris', 'germany'), ('dog', 'pet', 'cats', 'france', 'berlin')):\n",
    "    print(w1, w2, w2v.score(w1, w2))\n",
    "       \n",
    "for w1 in ['cat', 'dog', 'dogs', 'paris', 'germany']:\n",
    "    print(w2v.most_similar(w1),' are the most similar words for: ',w1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class BoV():\n",
    "    def __init__(self, w2v):\n",
    "        self.w2v = w2v\n",
    "\n",
    "\n",
    "    def encode(self, sentences, idf=False, fill_blank=False):\n",
    "        # takes a list of sentences, outputs a numpy array of sentence embeddings\n",
    "        # see TP1 for help\n",
    "        sentemb = []\n",
    "        for sent in sentences:\n",
    "            if idf is False:\n",
    "                encoding = [w2v.word2vec[w] for w in sent if w in w2v.word2vec]\n",
    "                if encoding != []:\n",
    "                    sentemb.append(np.mean(encoding, axis = 0))\n",
    "                #sentemb.append(np.mean([w2v.word2vec[w] for w in sent if w in w2v.word2vec], axis=0))\n",
    "                else:\n",
    "                    sentemb.append([0]*300)\n",
    "                #assert False, 'TODO: fill in the blank'\n",
    "                \n",
    "            else:\n",
    "                encoding = [w2v.word2vec[w]*idf[w] for w in sent if w in w2v.word2vec and w in idf]\n",
    "                if encoding != []:\n",
    "                    sentemb.append(np.mean(encoding, axis = 0))\n",
    "                else:\n",
    "                    sentemb.append([0]*300)\n",
    "                # idf-weighted mean of word vectors\n",
    "                #sentemb.append(np.mean([w2v.word2vec[w]*idf[w] for w in sent if w in w2v.word2vec and w in idf], axis=0))\n",
    "                #assert False, 'TODO: fill in the blank'\n",
    "        \n",
    "        sentemb_array = np.asarray(sentemb)\n",
    "        \n",
    "        return np.vstack(sentemb_array)\n",
    "        \n",
    "    def most_similar(self, s, sentences, idf=False, K=5):\n",
    "        # get most similar sentences and **print** them\n",
    "        index_s = sentences.index(s)\n",
    "        \n",
    "        #Encode all the sentences and retrieve the vector of the target sentence\n",
    "        keys = self.encode(sentences, idf)\n",
    "        query = keys[sentences.index(s),]  \n",
    "        \n",
    "        ## normalize embeddings\n",
    "        keys = keys / np.linalg.norm(keys, 2, 1)[:, None]  # normalize embeddings\n",
    "        query = query/np.linalg.norm(query)\n",
    "           \n",
    "        #compute score with other sentences vector for s            \n",
    "        scores = np.dot(keys,query) \n",
    "        idx = scores.argsort()[::-1][1:K+1]\n",
    "        \n",
    "        #select the K  biggest score(corresponding to the K most similar sentences with s)\n",
    "        sentences_most_similar = []\n",
    "        scores_most_similar = []\n",
    "        \n",
    "        for i in idx:\n",
    "            sentences_most_similar.append(sentences[i])\n",
    "            scores_most_similar.append(scores[i])\n",
    "             \n",
    "        return  print('The ',K,' most similar sentences for ',s,' are: \\n',sentences_most_similar, ',\\n With respective similarities: \\n',scores_most_similar)\n",
    "\n",
    "\n",
    "\n",
    "    def score(self, s1, s2, idf=False):\n",
    "    \n",
    "        #compute sentemp matrix\n",
    "        sentemb = self.encode(sentences,idf)\n",
    "        \n",
    "        #get back s1 and s2 encoded\n",
    "        s1_encoded = sentemb[sentences.index(s1)]\n",
    "        s2_encoded = sentemb[sentences.index(s2)]\n",
    "        \n",
    "        #normalization of s1 and s2 before doing dot product\n",
    "        s1_norm = np.linalg.norm(s1_encoded)\n",
    "        s2_norm = np.linalg.norm(s2_encoded)\n",
    "        \n",
    "        # dot-product of normalized vector = cosine similarity\n",
    "        s = np.dot(s1_encoded,s2_encoded)/float(s1_norm*s2_norm)\n",
    "       \n",
    "        return print('Similarity score between \\n',s1,'\\n and ',s2,'\\n is: ',s,'\\n')\n",
    "\n",
    "    \n",
    "    def build_idf(self,sentences):\n",
    "        # build the idf dictionary: associate each word to its idf value\n",
    "        idf = {} \n",
    "\n",
    "        for sent in sentences:\n",
    "                for w in set(sent): #set() gives unique elements in list \n",
    "                    idf[w] = idf.get(w, 0) + 1\n",
    "\n",
    "        for w in idf:\n",
    "            idf[w] = max(1,(np.log10(len(sentences) / (idf[w]))))\n",
    "            \n",
    "        return idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 200000 pretrained word vectors\n",
      "\n",
      " BoV-mean Model:\n",
      "The  5  most similar sentences for  ['1', 'smiling', 'african', 'american', 'boy', '.']  are: \n",
      " [['an', 'african', 'american', 'man', 'smiling', '.'], ['a', 'little', 'african', 'american', 'boy', 'and', 'girl', 'looking', 'up', '.'], ['an', 'afican', 'american', 'woman', 'standing', 'behind', 'two', 'small', 'african', 'american', 'children', '.'], ['an', 'african', 'american', 'man', 'is', 'sitting', '.'], ['a', 'girl', 'in', 'black', 'hat', 'holding', 'an', 'african', 'american', 'baby', '.']] ,\n",
      " With respective similarities: \n",
      " [0.91704533577077552, 0.84985560038758123, 0.82171391395408944, 0.82070473846335068, 0.81911224910118519]\n",
      "\n",
      "\n",
      "Similarity score between \n",
      " ['1', 'man', 'singing', 'and', '1', 'man', 'playing', 'a', 'saxophone', 'in', 'a', 'concert', '.'] \n",
      " and  ['10', 'people', 'venture', 'out', 'to', 'go', 'crosscountry', 'skiing', '.'] \n",
      " is:  0.572625885972 \n",
      "\n",
      "\n",
      " BoV-idf Model:\n",
      "The  5  most similar sentences for  ['1', 'smiling', 'african', 'american', 'boy', '.']  are: \n",
      " [['an', 'african', 'american', 'man', 'smiling', '.'], ['an', 'african', 'american', 'man', 'is', 'sitting', '.'], ['a', 'little', 'african', 'american', 'boy', 'and', 'girl', 'looking', 'up', '.'], ['an', 'afican', 'american', 'woman', 'standing', 'behind', 'two', 'small', 'african', 'american', 'children', '.'], ['a', 'girl', 'in', 'black', 'hat', 'holding', 'an', 'african', 'american', 'baby', '.']] ,\n",
      " With respective similarities: \n",
      " [0.92152306157765251, 0.87105600629737734, 0.86491606454660208, 0.85467877390550606, 0.84615786511924873]\n",
      "\n",
      "\n",
      "Similarity score between \n",
      " ['1', 'man', 'singing', 'and', '1', 'man', 'playing', 'a', 'saxophone', 'in', 'a', 'concert', '.'] \n",
      " and  ['10', 'people', 'venture', 'out', 'to', 'go', 'crosscountry', 'skiing', '.'] \n",
      " is:  0.475145087537 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "w2v = Word2vec(os.path.join(PATH_TO_DATA, 'crawl-300d-200k.vec'), nmax=200000)\n",
    "s2v = BoV(w2v)\n",
    "\n",
    "# Load sentences in \"PATH_TO_DATA/sentences.txt\"\n",
    "sentences = []\n",
    "with open(os.path.join(PATH_TO_DATA, 'sentences.txt')) as f:\n",
    "    for i, line in enumerate(f):\n",
    "        sent = line.rstrip().split()\n",
    "        sentences.append(sent)\n",
    "\n",
    "# Build idf scores for each word\n",
    "\n",
    "idf = {}  \n",
    "\n",
    "# You will be evaluated on the output of the following:\n",
    "print('\\n BoV-mean Model:')\n",
    "s2v.most_similar('' if not sentences else sentences[10], sentences)  # BoV-mean\n",
    "print('\\n')\n",
    "s2v.score('' if not sentences else sentences[7], '' if not sentences else sentences[13])\n",
    "\n",
    "\n",
    "idf = s2v.build_idf(sentences)\n",
    "print('\\n BoV-idf Model:')\n",
    "s2v.most_similar('' if not sentences else sentences[10], sentences, idf)  # BoV-idf\n",
    "print('\\n')\n",
    "s2v.score('' if not sentences else sentences[7], '' if not sentences else sentences[13], idf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Multilingual (English-French) word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider a bilingual dictionary of size V_a (e.g French-English).\n",
    "\n",
    "Let's define **X** and **Y** the **French** and **English** matrices.\n",
    "\n",
    "They contain the embeddings associated to the words in the bilingual dictionary.\n",
    "\n",
    "We want to find a **mapping W** that will project the source word space (e.g French) to the target word space (e.g English).\n",
    "\n",
    "Procrustes : **W\\* = argmin || W.X - Y ||  s.t  W^T.W = Id**\n",
    "has a closed form solution:\n",
    "**W = U.V^T  where  U.Sig.V^T = SVD(Y.X^T)**\n",
    "\n",
    "In what follows, you are asked to: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_fr and dict_en created, with respective length  50000  and  50000\n"
     ]
    }
   ],
   "source": [
    "# 1 - Download and load 50k first vectors of\n",
    "data_en = \"https://s3-us-west-1.amazonaws.com/fasttext-vectors/wiki.en.vec\"\n",
    "data_fr = \"https://s3-us-west-1.amazonaws.com/fasttext-vectors/wiki.fr.vec\"\n",
    "\n",
    "# TYPE CODE HERE\n",
    "import urllib.request as url\n",
    "\n",
    "def load(path, limit = 50000):\n",
    "    dic = {}\n",
    "    with url.urlopen(path) as f:\n",
    "        next(f)\n",
    "        for i, line in enumerate(f):\n",
    "            word, vec = line.decode('utf-8').split(' ', 1)\n",
    "            dic[word] = np.fromstring(vec, sep=' ')\n",
    "            if i == (limit-1):\n",
    "                break\n",
    "    return dic\n",
    "\n",
    "dict_fr = load(data_fr)\n",
    "dict_en = load(data_en)\n",
    "\n",
    "\n",
    "print('dict_fr and dict_en created, with respective length ',len(dict_fr),' and ',len(dict_en))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 matrices X and Y were generated: \n",
      "X.shape =  (18970, 300) \n",
      "Y.shape =  (18970, 300)\n"
     ]
    }
   ],
   "source": [
    "# 2 - Get words that appear in both vocabs (= identical character strings)\n",
    "#     Use it to create the matrix X and Y (of aligned embeddings for these words)\n",
    "\n",
    "def create_matrices(dict_1,dict_2):\n",
    "    list_X = []\n",
    "    list_Y = []\n",
    "    for w in dict_1:\n",
    "        if w in dict_2:\n",
    "            list_X.append(dict_1[w])\n",
    "            list_Y.append(dict_2[w])\n",
    "    X = np.vstack(list_X)\n",
    "    Y = np.vstack(list_Y)\n",
    "    return X,Y\n",
    "\n",
    "#Call the function to create matrices based en shared words\n",
    "X, Y = create_matrices(dict_fr,dict_en)\n",
    "\n",
    "print('2 matrices X and Y were generated: \\nX.shape = ',X.shape,'\\nY.shape = ',Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Yt . X:  (300, 300)\n",
      "Singular value decomposition gives: SVD(Yt . X) = U . S . Vt\n",
      "Shape of U:  (300, 300) ,\n",
      "Shape of s:  (300,) ,\n",
      "Shape of Vh:  (300, 300)\n",
      "\n",
      "W is generated \n",
      "Shape of W = U.Vt :  (300, 300)\n"
     ]
    }
   ],
   "source": [
    "# 3 - Solve the Procrustes using the scipy package and: scipy.linalg.svd() and get the optimal W\n",
    "#     Now W*French_vector is in the same space as English_vector\n",
    "\n",
    "from scipy.linalg import svd\n",
    "#from scipy.linalg import orthogonal_procrustes\n",
    "\n",
    "Xt_Y = np.dot(X.T,Y)\n",
    "print('Shape of Yt . X: ',Xt_Y.shape) # N*N\n",
    "\n",
    "print('Singular value decomposition gives: SVD(Yt . X) = U . S . Vt')\n",
    "U, S, Vt = scipy.linalg.svd(Xt_Y)\n",
    "print('Shape of U: ',U.shape, ',\\nShape of s: ',S.shape, ',\\nShape of Vh: ',Vt.shape )\n",
    "\n",
    "W = np.dot(U,Vt)\n",
    "print('\\nW is generated \\nShape of W = U.Vt : ',W.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "French to English equivalents:\n",
      "\n",
      " Top-5 neighbors of \"mot\"\n",
      "['phrase', 'noun', 'meaning', 'words', 'adjective']\n",
      "\n",
      " Top-5 neighbors of \"voisin\"\n",
      "['neighbor', 'neighbouring', 'neighboring', 'neighbors', 'nearby']\n",
      "\n",
      " Top-5 neighbors of \"proche\"\n",
      "['closer', 'distant', 'close', 'distantly', 'nearer']\n",
      "\n",
      " English to French equivalents:\n",
      "\n",
      " Top-5 neighbors of \"word\"\n",
      "['substantif', 'signifiant', 'locution', 'mots', 'étymologiquement']\n",
      "\n",
      " Top-5 neighbors of \"neighbors\"\n",
      "['voisin', 'voisinage', 'peuplades', 'houses', 'our']\n",
      "\n",
      " Top-5 neighbors of \"nearest\"\n",
      "['distance', 'km', 'kilomètre', 'distant', 'desservie']\n"
     ]
    }
   ],
   "source": [
    "# 4 - After alignment with W, give examples of English nearest neighbors of some French words (and vice versa)\n",
    "#     You will be evaluated on that part and the code above\n",
    "\n",
    "#Function to compute cos similarity between two vectors, like in exercise 1\n",
    "\n",
    "def score(vect1, vect2):\n",
    "    dot_product = np.dot(vect1, vect2.T)\n",
    "    return dot_product / float(np.linalg.norm(vect1) * np.linalg.norm(vect2))\n",
    "\n",
    "\n",
    "#Function to find nearest neighbors [K is customisable]\n",
    "\n",
    "def find_neighbors(target_word, W, original_dict, target_dict, K=5):\n",
    "    target_vect = original_dict[target_word]\n",
    "    equiv_word = np.dot(target_vect,W)\n",
    "    scores = []\n",
    "    labels = list(target_dict.keys())\n",
    "    \n",
    "    for word in labels:\n",
    "        scores.append(score(equiv_word,target_dict[word]))\n",
    "    scores_array = np.array(scores)\n",
    "    idxs = scores_array.argsort()[::-1][1:K+1]\n",
    "\n",
    "    print('\\n Top-%s neighbors of \"%s\"' % (K, target_word))\n",
    "    similar_words = []\n",
    "    for idx in idxs:\n",
    "        similar_words.append(labels[idx])\n",
    "    return similar_words\n",
    "    \n",
    "print('French to English equivalents:')\n",
    "for w1 in ['mot', 'voisin', 'proche']:\n",
    "    print(find_neighbors(w1, W, dict_fr, dict_en, K=5))\n",
    "\n",
    "print('\\n English to French equivalents:')\n",
    "for w1 in ['word', 'neighbors', 'nearest']:\n",
    "    print(find_neighbors(w1, W.T, dict_en, dict_fr, K=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to dive deeper on this subject: https://github.com/facebookresearch/MUSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) Sentence classification with BoV and scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 1 - Load train/dev/test of Stanford Sentiment TreeBank (SST)\n",
    "#     (https://nlp.stanford.edu/~socherr/EMNLP2013_RNTN.pdf)\n",
    "\n",
    "PATH_TO_SST = \"C:/Users/Chuyi/Documents/DSBA/T2/Deep_Learning/Mini_project_2/nlp_project/data/SST/\"\n",
    "\n",
    "\n",
    "#Load the sentences like in the 1st exercise\n",
    "def load(path, label_cut=False):\n",
    "    labels = []\n",
    "    sentences = []\n",
    "    with open(path, encoding=\"utf8\") as f:\n",
    "        for line in f:\n",
    "            \n",
    "            if label_cut == False:\n",
    "                label, line = line.split(' ', 1)\n",
    "                labels.append(label)\n",
    "                \n",
    "            sentences.append(line.split())\n",
    "    return labels, sentences\n",
    "\n",
    "#Load labeled data\n",
    "Y_train , X_train = load(os.path.join(PATH_TO_SST,'stsa.fine.train'), label_cut = False)\n",
    "Y_dev , X_dev = load(os.path.join(PATH_TO_SST,'stsa.fine.dev'), label_cut = False)\n",
    "\n",
    "#Load non-labeled data\n",
    "Y_test , X_test = load(os.path.join(PATH_TO_SST,'stsa.fine.test.X'), label_cut = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 - Encode sentences with the BoV model above\n",
    "# TYPE CODE HERE\n",
    "\n",
    "#First model doesn't need idf weights\n",
    "idf_3 = {}\n",
    "W_train = s2v.encode(X_train)\n",
    "W_dev = s2v.encode(X_dev)\n",
    "W_test = s2v.encode(X_test)\n",
    "\n",
    "#Generate idf weights for encoding function\n",
    "idf_3 = s2v.build_idf(X_train)\n",
    "W_train_idf = s2v.encode(X_train ,idf_3)\n",
    "W_dev_idf = s2v.encode(X_dev, idf_3)\n",
    "W_test_idf = s2v.encode(X_test, idf_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BoV-mean model \n",
      "CV (=5) results for Simple LogisticRegression \n",
      " [ 0.4126242   0.42548217  0.44236396  0.42706503  0.43434936]\n",
      "CV results mean =  0.42837694153\n",
      "\n",
      "CV results on dev sets =  0.439600363306 \n",
      "\n",
      "BoV-idf model \n",
      "CV (=5) results for Simple LogisticRegression \n",
      " [ 0.40677966  0.40210403  0.42480983  0.40890451  0.42555686]\n",
      "CV results mean =  0.413630978608\n",
      "\n",
      "CV results on dev sets =  0.41961852861 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3 - Learn Logistic Regression on top of sentence embeddings using scikit-learn\n",
    "#     (consider tuning the L2 regularization on the dev set)\n",
    "\n",
    "# TYPE CODE HERE\n",
    "logit = LogisticRegression(penalty = 'l2',C=5)\n",
    "\n",
    "#Generate Cross validation results on training set with BoV mean model\n",
    "CV_Scr_mean = cross_val_score(logit,W_train,Y_train,cv=5)\n",
    "\n",
    "print('BoV-mean model \\nCV (=5) results for Simple LogisticRegression \\n',CV_Scr_mean)\n",
    "print('CV results mean = ',np.mean(CV_Scr_mean))\n",
    "\n",
    "#Test with dev set with BoV mean model\n",
    "logit.fit(W_train,Y_train)\n",
    "\n",
    "print('\\nCV results on dev sets = ',logit.score(W_dev, Y_dev),'\\n')\n",
    "\n",
    "#Generate Cross validation results on training set with Bov idf model\n",
    "CV_Scr_idf = cross_val_score(logit,W_train_idf,Y_train,cv=5)\n",
    "\n",
    "print('BoV-idf model \\nCV (=5) results for Simple LogisticRegression \\n',CV_Scr_idf)\n",
    "print('CV results mean = ',np.mean(CV_Scr_idf))\n",
    "\n",
    "#Test with dev set with BoV idf model\n",
    "logit.fit(W_train_idf,Y_train)\n",
    "\n",
    "print('\\nCV results on dev sets = ',logit.score(W_dev_idf, Y_dev),'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 0.438436329588015\n",
      "Best parameters: {'C': 1}\n",
      "0.439600363306\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "\n",
    "# turn run_gs to True if you want to run the gridsearch again.\n",
    "run_gs = True\n",
    "\n",
    "if run_gs:\n",
    "    parameter_grid = {\n",
    "                 'C': [0.001, 0.01, 0.1, 1, 5, 10, 100, 1000]\n",
    "                 }\n",
    "    lr = LogisticRegression()\n",
    "    cross_validation = StratifiedKFold(Y_train, n_folds=5)\n",
    "\n",
    "    grid_search = GridSearchCV(lr,\n",
    "                               scoring='accuracy',\n",
    "                               param_grid=parameter_grid,\n",
    "                               cv=cross_validation)\n",
    "\n",
    "    grid_search.fit(W_train, Y_train)\n",
    "    lr_best_avg = grid_search\n",
    "    parameters = grid_search.best_params_\n",
    "\n",
    "    print('Best score: {}'.format(grid_search.best_score_))\n",
    "    print('Best parameters: {}'.format(grid_search.best_params_))\n",
    "else: \n",
    "    parameters = {'C': 1.0}\n",
    "    \n",
    "    lr_best_avg = LogisticRegression(**parameters)\n",
    "    lr_best_avg.fit(W_train, Y_train)\n",
    "    \n",
    "print(lr_best_avg.score(W_dev,Y_dev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 4 - Produce 2210 predictions for the test set (in the same order). One line = one prediction (=0,1,2,3,4).\n",
    "#     Attach the output file \"logreg_bov_y_test_sst.txt\" to your deliverable.\n",
    "#     You will be evaluated on the results of the test set.\n",
    "\n",
    "# TYPE CODE HERE\n",
    "\n",
    "Y_pred_test = logit.predict(W_test)\n",
    "\n",
    "text_file = open(\"logreg_bov_y_test_sst.txt\", \"w\")\n",
    "text_file.write('\\n'.join(Y_pred_test))\n",
    "text_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BoV-mean model \n",
      "CV results for SVM [ 0.39158387  0.38281707  0.37097718  0.40187463  0.38335287]\n",
      "CV mean =  0.386121124168 \n",
      "\n",
      "CV results on dev sets = 0.405994550409 \n",
      "\n",
      "BoV-idf model \n",
      "CV results for SVM [ 0.37463472  0.38924605  0.36863663  0.39660223  0.34583822]\n",
      "CV mean =  0.374991569054 \n",
      "\n",
      "CV results on dev sets = 0.3851044505 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# BONUS!\n",
    "# 5 - Try to improve performance with another classifier\n",
    "#     Attach the output file \"XXX_bov_y_test_sst.txt\" to your deliverable (where XXX = the name of the classifier)\n",
    "\n",
    "# TYPE CODE HERE\n",
    "\n",
    "# Support Vector Machines\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "clf = GaussianNB()\n",
    "\n",
    "# Dev test \n",
    "\n",
    "CV_score = cross_val_score(clf,W_train,Y_train,cv=5)\n",
    "print('BoV-mean model \\nCV results for SVM',CV_score)\n",
    "print('CV mean = ', np.mean(CV_score),'\\n')\n",
    "clf.fit(W_train,Y_train)\n",
    "print('CV results on dev sets =',clf.score(W_dev, Y_dev),'\\n')\n",
    "\n",
    "CV_score_idf = cross_val_score(clf,W_train_idf,Y_train,cv=5)\n",
    "print('BoV-idf model \\nCV results for SVM',CV_score_idf)\n",
    "print('CV mean = ', np.mean(CV_score_idf),'\\n')\n",
    "clf.fit(W_train_idf,Y_train)\n",
    "print('CV results on dev sets =',clf.score(W_dev_idf, Y_dev),'\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output_file = open(\"XXX_bov_y_test_sst.txt.txt\",'w')\n",
    "output_file.write('\\n'.join(Y_pred))\n",
    "output_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4) Sentence classification with LSTMs in Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 - Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 - Load train/dev/test sets of SST\n",
    "\n",
    "PATH_TO_SST = \"C:/Users/Chuyi/Documents/DSBA/T2/Deep_Learning/Mini_project_2/nlp_project/data/SST/\"\n",
    "\n",
    "#Load the sentences like in the 1st exercise\n",
    "def load_texts(path, label_cut=False):\n",
    "    labels = []\n",
    "    sentences = []\n",
    "    with open(path, encoding=\"utf8\") as f:\n",
    "        for line in f:\n",
    "            if label_cut == False:\n",
    "                label, line = line.split(' ', 1)\n",
    "                labels.append(label)\n",
    "            sentences.append(line.split())\n",
    "    return labels, sentences\n",
    "\n",
    "#Load labeled data\n",
    "Y_train , X_train = load_texts(os.path.join(PATH_TO_SST,'stsa.fine.train'), label_cut = False)\n",
    "Y_dev , X_dev = load_texts(os.path.join(PATH_TO_SST,'stsa.fine.dev'), label_cut = False)\n",
    "\n",
    "#Load non-labeled data\n",
    "Y_test , X_test = load_texts(os.path.join(PATH_TO_SST,'stsa.fine.test.X'), label_cut = True)\n",
    "\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "Y_train = to_categorical(Y_train,5)\n",
    "Y_dev = to_categorical(Y_dev,5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 2 - Transform text to integers using keras.preprocessing.text.one_hot function\n",
    "#     https://keras.io/preprocessing/text/\n",
    "\n",
    "#from keras.preprocessing.text import text_to_word_sequence\n",
    "from keras.preprocessing.text import one_hot\n",
    "\n",
    "def text_to_integers(text, size):\n",
    "    output=[]\n",
    "    for sent in text:\n",
    "        sent = str(sent)\n",
    "        new = keras.preprocessing.text.one_hot(sent, \n",
    "                                    size,\n",
    "                                     filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n",
    "                                     lower=True,\n",
    "                                     split=\" \")\n",
    "        output.append(new)\n",
    "    return output\n",
    "\n",
    "size_voc = 16000\n",
    "X_train_processed = text_to_integers(X_train, size_voc)\n",
    "X_dev_processed = text_to_integers(X_dev, size_voc)\n",
    "X_test_processed = text_to_integers(X_test, size_voc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Padding input data**\n",
    "\n",
    "Models in Keras (and elsewhere) take batches of sentences of the same length as input. It is because Deep Learning framework have been designed to handle well Tensors, which are particularly suited for fast computation on the GPU.\n",
    "\n",
    "Since sentences have different sizes, we \"pad\" them. That is, we add dummy \"padding\" tokens so that they all have the same length.\n",
    "\n",
    "The input to a Keras model thus has this size : (batchsize, maxseqlen) where maxseqlen is the maximum length of a sentence in the batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67\n"
     ]
    }
   ],
   "source": [
    "# 3 - Pad your sequences using keras.preprocessing.sequence.pad_sequences\n",
    "#     https://keras.io/preprocessing/sequence/\n",
    "\n",
    "# TYPE CODE HERE\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "X_train_4 = pad_sequences(X_train_processed)\n",
    "X_dev_4 = pad_sequences(X_dev_processed)\n",
    "X_test_4 =pad_sequences(X_test_processed)\n",
    "\n",
    "print(len(X_train_4[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 - Design and train your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:20: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(64, dropout=0.2, recurrent_dropout=0.2)`\n"
     ]
    }
   ],
   "source": [
    "# 4 - Design your encoder + classifier using keras.layers\n",
    "#     In Keras, Torch and other deep learning framework, we create a \"container\" which is the Sequential() module.\n",
    "#     Then we add components to this contained : the lookuptable, the LSTM, the classifier etc.\n",
    "#     All of these components are contained in the Sequential() and are trained together.\n",
    "\n",
    "\n",
    "# ADAPT CODE BELOW\n",
    "\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense, Activation\n",
    "\n",
    "embed_dim  = 2000  # word embedding dimension\n",
    "nhid       = 64  # number of hidden units in the LSTM\n",
    "vocab_size = size_voc  # size of the vocabulary\n",
    "n_classes  = 5\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, embed_dim))\n",
    "model.add(LSTM(nhid, dropout_W=0.2, dropout_U=0.2))\n",
    "model.add(Dense(n_classes, activation='softmax'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_10 (Embedding)     (None, None, 2000)        32000000  \n",
      "_________________________________________________________________\n",
      "lstm_10 (LSTM)               (None, 64)                528640    \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 5)                 325       \n",
      "=================================================================\n",
      "Total params: 32,528,965\n",
      "Trainable params: 32,528,965\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# 5 - Define your loss/optimizer/metrics\n",
    "\n",
    "# MODIFY CODE BELOW\n",
    "\n",
    "loss_classif     =  'categorical_crossentropy' # find the right loss for multi-class classification\n",
    "optimizer        =  'rmsprop' # find the right optimizer\n",
    "metrics_classif  =  ['accuracy']\n",
    "\n",
    "# Observe how easy (but blackboxed) this is in Keras\n",
    "model.compile(loss=loss_classif,\n",
    "              optimizer=optimizer,\n",
    "              metrics=metrics_classif)\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\models.py:944: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8544 samples, validate on 1101 samples\n",
      "Epoch 1/4\n",
      "8544/8544 [==============================] - 166s 19ms/step - loss: 1.4999 - acc: 0.3270 - val_loss: 1.4096 - val_acc: 0.3869\n",
      "Epoch 2/4\n",
      "8544/8544 [==============================] - 167s 20ms/step - loss: 1.2058 - acc: 0.4912 - val_loss: 1.4166 - val_acc: 0.4033\n",
      "Epoch 3/4\n",
      "8544/8544 [==============================] - 169s 20ms/step - loss: 0.9101 - acc: 0.6440 - val_loss: 1.5879 - val_acc: 0.3706\n",
      "Epoch 4/4\n",
      "8544/8544 [==============================] - 170s 20ms/step - loss: 0.6583 - acc: 0.7512 - val_loss: 1.8391 - val_acc: 0.3860\n"
     ]
    }
   ],
   "source": [
    "# 6 - Train your model and find the best hyperparameters for your dev set\n",
    "#     you will be evaluated on the quality of your predictions on the test set\n",
    "\n",
    "# ADAPT CODE BELOW\n",
    "bs = 64\n",
    "n_epochs = 4\n",
    "\n",
    "LSTM = model.fit(X_train_4, Y_train, batch_size=bs, nb_epoch=n_epochs, validation_data=(X_dev_4, Y_dev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEICAYAAACktLTqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x221db3581d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEICAYAAACktLTqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x221db36ca90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Plot the evolution of train/dev results w.r.t the number of epochs\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "plt.title(\"Accuracy\")\n",
    "plt.plot(LSTM.history[\"acc\"], color=\"g\", label=\"Train\")\n",
    "plt.plot(LSTM.history[\"val_acc\"], color=\"y\", label=\"Dev\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.show()\n",
    "\n",
    "plt.title(\"Loss\")\n",
    "plt.plot(LSTM.history[\"loss\"], color=\"g\", label=\"Train\")\n",
    "plt.plot(LSTM.history[\"val_loss\"], color=\"y\", label=\"Dev\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 7 - Generate your predictions on the test set using model.predict(x_test)\n",
    "#     https://keras.io/models/model/\n",
    "#     Log your predictions in a file (one line = one integer: 0,1,2,3,4)\n",
    "#     Attach the output file \"logreg_lstm_y_test_sst.txt\" to your deliverable.\n",
    "\n",
    "Y_test_pred = model.predict(X_test_4)\n",
    "Y_pred = []\n",
    "\n",
    "\n",
    "for i in range(Y_test_pred.shape[0]):\n",
    "    index_max = list(Y_test_pred[i]).index(max(Y_test_pred[i]))\n",
    "    Y_pred.append(str(index_max))\n",
    "Y_pred = np.asarray(Y_pred)\n",
    "\n",
    "#create file with predictions\n",
    "output_file = open(\"logreg_lstm_y_test_sst.txt.txt\",'w')\n",
    "output_file.write('\\n'.join(Y_pred))\n",
    "output_file.close()\n",
    "\n",
    "#print(Y_test_pred[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 -- innovate !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:20: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(64, dropout=0.2, recurrent_dropout=0.2)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, None, 2000)        4000000   \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 64)                528640    \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 5)                 325       \n",
      "=================================================================\n",
      "Total params: 4,528,965\n",
      "Trainable params: 4,528,965\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\models.py:944: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8544 samples, validate on 1101 samples\n",
      "Epoch 1/4\n",
      "8544/8544 [==============================] - 137s 16ms/step - loss: 1.5306 - acc: 0.3141 - val_loss: 1.4655 - val_acc: 0.3524\n",
      "Epoch 2/4\n",
      "8544/8544 [==============================] - 122s 14ms/step - loss: 1.3283 - acc: 0.4341 - val_loss: 1.4871 - val_acc: 0.3488\n",
      "Epoch 3/4\n",
      "8544/8544 [==============================] - 120s 14ms/step - loss: 1.1556 - acc: 0.5302 - val_loss: 1.5657 - val_acc: 0.3433\n",
      "Epoch 4/4\n",
      "8544/8544 [==============================] - 120s 14ms/step - loss: 0.9859 - acc: 0.6040 - val_loss: 1.7428 - val_acc: 0.3261\n"
     ]
    }
   ],
   "source": [
    "# 8 - Open question: find a model that is better on your dev set\n",
    "#     (e.g: use a 1D ConvNet, use a better classifier, pretrain your lookup tables ..)\n",
    "#     you will get point if the results on the test set are better: be careful of not overfitting your dev set too much..\n",
    "#     Attach the output file \"XXX_XXX_y_test_sst.txt\" to your deliverable.\n",
    "\n",
    "# TYPE CODE HERE\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense, Activation\n",
    "\n",
    "embed_dim  = 2000  # word embedding dimension\n",
    "nhid       = 64  # number of hidden units in the LSTM\n",
    "vocab_size = size_voc  # size of the vocabulary\n",
    "n_classes  = 5\n",
    "\n",
    "model2 = Sequential()\n",
    "\n",
    "model2.add(Embedding(vocab_size, embed_dim))\n",
    "\n",
    "model2.add(LSTM(nhid, dropout_W=0.2, dropout_U=0.2))\n",
    "\n",
    "model2.add(Dense(n_classes, activation='softmax'))\n",
    "\n",
    "\n",
    "loss_classif     =  'categorical_crossentropy' # find the right loss for multi-class classification\n",
    "optimizer        =  'Adam' # find the right optimizer\n",
    "metrics_classif  =  ['accuracy']\n",
    "\n",
    "# Observe how easy (but blackboxed) this is in Keras\n",
    "model2.compile(loss=loss_classif,\n",
    "              optimizer=optimizer,\n",
    "              metrics=metrics_classif)\n",
    "print(model2.summary())\n",
    "\n",
    "bs = 64\n",
    "n_epochs = 4\n",
    "\n",
    "LSTM = model2.fit(X_train_4, Y_train, batch_size=bs, nb_epoch=n_epochs, validation_data=(X_dev_4, Y_dev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output_file = open(\"XXX_XXX_y_test_sst.txt.txt\",'w')\n",
    "output_file.write('\\n'.join(Y_pred))\n",
    "output_file.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
